{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Segmentation_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgdtop/test2/blob/main/Segmentation_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vegbFIH8DMVk"
      },
      "source": [
        "<br> ======== \n",
        "<br> Initial: Jong-Min Kim, Ph. D., 24-April-2020\n",
        "<br> ======== \n",
        "<br> Modified: Jong-Min Kim, Ph. D., 15-June-2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JFE4A24DVxL"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqIUR3UlCuD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d4b21bd-7163-41e3-9c6e-c11fc13b02c0"
      },
      "source": [
        "!pip install SimpleITK\n",
        "!pip install runstats\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting SimpleITK\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/cb/a15f4612af8e37f3627fc7fb2f91d07bb584968b0a47e3d5103d7014f93e/SimpleITK-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (44.9MB)\n",
            "\u001b[K     |████████████████████████████████| 44.9MB 127kB/s \n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.0.1\n",
            "Collecting runstats\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/6e/c9b0812b41e9625e09cd900205a1482c07d6c9c3405f8a1ddc3bf576e7a7/runstats-1.8.0.tar.gz (110kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 4.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: runstats\n",
            "  Building wheel for runstats (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for runstats: filename=runstats-1.8.0-cp36-cp36m-linux_x86_64.whl size=200039 sha256=8542e858470673a42c7eb61c01aeec35724efc58404ce6b51a60aef0fdaba065\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/f0/2b/509c185283b7a501bcad14d5ab99fd614c80dcb1f894897d0e\n",
            "Successfully built runstats\n",
            "Installing collected packages: runstats\n",
            "Successfully installed runstats-1.8.0\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.2)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjqoPT7yDbu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "000751ee-89a8-4859-8a22-7614cdd42646"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_k0KsVADgN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1280f73c-9af8-4e31-90b7-f713ff80f5b9"
      },
      "source": [
        "!python '/content/gdrive/My Drive/WIP/EIEN-2020-Fall/EIEN394/Lecture_for_me/Lecture 17 - Exp/Toy code for segmentation/Segmentation_using_DL/Segm_DL_playground-master_v3/train.py' --num-pools 6 --num-chans 8 --accel_method 'gpu' --batch-size 1 --num-epoch 10 --data-path '/content/gdrive/My Drive/WIP/EIEN-2020-Fall/EIEN394/Lecture_for_me/Lecture 17 - Exp/Toy code for segmentation/TrainingData_Splitted' --drop-prob 0.1 --exp-dir '/content/gdrive/My Drive/WIP/EIEN-2020-Fall/EIEN394/Lecture_for_me/Lecture 17 - Exp/Toy code for segmentation/Segmentation_using_DL/ckt' "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:Namespace(InChans=1, OutChans=2, accel_method='gpu', batch_size=1, checkpoint=None, data_parallel=False, data_path=PosixPath('/content/gdrive/My Drive/WIP/EIEN-2020-Fall/EIEN394/Lecture_for_me/Lecture 17 - Exp/Toy code for segmentation/TrainingData_Splitted'), device='cuda', drop_prob=0.1, exp_dir=PosixPath('/content/gdrive/My Drive/WIP/EIEN-2020-Fall/EIEN394/Lecture_for_me/Lecture 17 - Exp/Toy code for segmentation/Segmentation_using_DL/ckt'), lr=0.0005, lr_gamma=0.1, lr_step_size=10, naming='PROMISE12_h5', network_model='unet', num_cascades=5, num_chans=8, num_epochs=10, num_pools=6, report_interval=50, resolution=256, resume=False, sample_rate=1.0, seed=42, weigh_L1=0, weigh_L_BCE=1, weigh_L_dice=0, weight_decay=0.0)\n",
            "INFO:root:build_model(\n",
            "  (model): UnetModel(\n",
            "    (down_sample_layers): ModuleList(\n",
            "      (0): ConvBlock(in_chans=1, out_chans=8, drop_prob=0.1)\n",
            "      (1): ConvBlock(in_chans=8, out_chans=16, drop_prob=0.1)\n",
            "      (2): ConvBlock(in_chans=16, out_chans=32, drop_prob=0.1)\n",
            "      (3): ConvBlock(in_chans=32, out_chans=64, drop_prob=0.1)\n",
            "      (4): ConvBlock(in_chans=64, out_chans=128, drop_prob=0.1)\n",
            "      (5): ConvBlock(in_chans=128, out_chans=256, drop_prob=0.1)\n",
            "    )\n",
            "    (conv): ConvBlock(in_chans=256, out_chans=512, drop_prob=0.1)\n",
            "    (up_conv): ModuleList(\n",
            "      (0): ConvBlock(in_chans=512, out_chans=256, drop_prob=0.1)\n",
            "      (1): ConvBlock(in_chans=256, out_chans=128, drop_prob=0.1)\n",
            "      (2): ConvBlock(in_chans=128, out_chans=64, drop_prob=0.1)\n",
            "      (3): ConvBlock(in_chans=64, out_chans=32, drop_prob=0.1)\n",
            "      (4): ConvBlock(in_chans=32, out_chans=16, drop_prob=0.1)\n",
            "      (5): Sequential(\n",
            "        (0): ConvBlock(in_chans=16, out_chans=8, drop_prob=0.1)\n",
            "        (1): Conv2d(8, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (up_transpose_conv): ModuleList(\n",
            "      (0): ConvBlock(in_chans=512, out_chans=256)\n",
            "      (1): ConvBlock(in_chans=256, out_chans=128)\n",
            "      (2): ConvBlock(in_chans=128, out_chans=64)\n",
            "      (3): ConvBlock(in_chans=64, out_chans=32)\n",
            "      (4): ConvBlock(in_chans=32, out_chans=16)\n",
            "      (5): ConvBlock(in_chans=16, out_chans=8)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [   0/1137] Loss = 1.031 Avg Loss = 1.031 Time = 1.3585s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [  50/1137] Loss = 0.8932 Avg Loss = 0.9759 Time = 0.0563s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 100/1137] Loss = 0.9294 Avg Loss = 0.9149 Time = 0.0715s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 150/1137] Loss = 0.7281 Avg Loss = 0.8586 Time = 0.0520s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 200/1137] Loss = 0.6818 Avg Loss = 0.809 Time = 0.0559s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 250/1137] Loss = 0.7548 Avg Loss = 0.7601 Time = 0.0617s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 300/1137] Loss = 0.5463 Avg Loss = 0.7024 Time = 0.0765s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 350/1137] Loss = 0.5162 Avg Loss = 0.6478 Time = 0.0618s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 400/1137] Loss = 0.4864 Avg Loss = 0.5962 Time = 0.0517s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 450/1137] Loss = 0.3909 Avg Loss = 0.5449 Time = 0.0535s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 500/1137] Loss = 0.3176 Avg Loss = 0.4935 Time = 0.0559s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 550/1137] Loss = 0.2588 Avg Loss = 0.4291 Time = 0.0561s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 600/1137] Loss = 0.2865 Avg Loss = 0.3834 Time = 0.0550s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 650/1137] Loss = 0.2831 Avg Loss = 0.3213 Time = 0.0577s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 700/1137] Loss = 0.099 Avg Loss = 0.2867 Time = 0.0686s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 750/1137] Loss = 0.1495 Avg Loss = 0.2339 Time = 0.0354s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 800/1137] Loss = 0.2313 Avg Loss = 0.1784 Time = 0.0759s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 850/1137] Loss = 0.1464 Avg Loss = 0.1486 Time = 0.0711s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 900/1137] Loss = 0.01976 Avg Loss = 0.1273 Time = 0.0833s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [ 950/1137] Loss = 0.1823 Avg Loss = 0.1066 Time = 0.0828s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [1000/1137] Loss = 0.3111 Avg Loss = 0.09846 Time = 0.0588s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [1050/1137] Loss = 0.04997 Avg Loss = 0.08238 Time = 0.0817s\n",
            "INFO:root:Epoch = [  0/ 10] Iter = [1100/1137] Loss = 0.2564 Avg Loss = 0.09112 Time = 0.0851s\n",
            "INFO:root:Epoch = [   0/  10] TrainLoss = 0.08164 ValLoss = 0.05814 TrainTime = 73.2724s ValTime = 5.0338s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [   0/1137] Loss = 0.04639 Avg Loss = 0.04639 Time = 0.7959s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [  50/1137] Loss = 0.04152 Avg Loss = 0.0601 Time = 0.0581s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 100/1137] Loss = 0.0647 Avg Loss = 0.06686 Time = 0.0719s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 150/1137] Loss = 0.2359 Avg Loss = 0.06902 Time = 0.0830s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 200/1137] Loss = 0.0453 Avg Loss = 0.07482 Time = 0.0815s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 250/1137] Loss = 0.3239 Avg Loss = 0.07337 Time = 0.0587s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 300/1137] Loss = 0.04647 Avg Loss = 0.07539 Time = 0.0642s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 350/1137] Loss = 0.105 Avg Loss = 0.07938 Time = 0.0533s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 400/1137] Loss = 0.08043 Avg Loss = 0.08183 Time = 0.0611s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 450/1137] Loss = 0.08544 Avg Loss = 0.08515 Time = 0.0659s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 500/1137] Loss = 0.2512 Avg Loss = 0.08451 Time = 0.0758s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 550/1137] Loss = 0.08051 Avg Loss = 0.08235 Time = 0.0557s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 600/1137] Loss = 0.2622 Avg Loss = 0.08004 Time = 0.0617s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 650/1137] Loss = 0.1461 Avg Loss = 0.08675 Time = 0.0533s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 700/1137] Loss = 0.05279 Avg Loss = 0.07833 Time = 0.0679s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 750/1137] Loss = 0.07163 Avg Loss = 0.07428 Time = 0.0501s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 800/1137] Loss = 0.01239 Avg Loss = 0.08408 Time = 0.0542s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 850/1137] Loss = 0.05348 Avg Loss = 0.08313 Time = 0.0685s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 900/1137] Loss = 0.02164 Avg Loss = 0.08277 Time = 0.0563s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [ 950/1137] Loss = 0.08582 Avg Loss = 0.08497 Time = 0.0456s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [1000/1137] Loss = 0.05061 Avg Loss = 0.08176 Time = 0.0633s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [1050/1137] Loss = 0.07192 Avg Loss = 0.0812 Time = 0.0532s\n",
            "INFO:root:Epoch = [  1/ 10] Iter = [1100/1137] Loss = 0.03762 Avg Loss = 0.07895 Time = 0.0695s\n",
            "INFO:root:Epoch = [   1/  10] TrainLoss = 0.07278 ValLoss = 0.07388 TrainTime = 72.1070s ValTime = 5.1006s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [   0/1137] Loss = 0.02208 Avg Loss = 0.02208 Time = 0.8294s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [  50/1137] Loss = 0.05049 Avg Loss = 0.05142 Time = 0.0980s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 100/1137] Loss = 0.01544 Avg Loss = 0.06359 Time = 0.0811s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 150/1137] Loss = 0.06601 Avg Loss = 0.06991 Time = 0.0613s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 200/1137] Loss = 0.05852 Avg Loss = 0.07697 Time = 0.0613s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 250/1137] Loss = 0.05874 Avg Loss = 0.07731 Time = 0.0825s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 300/1137] Loss = 0.07267 Avg Loss = 0.08403 Time = 0.0567s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 350/1137] Loss = 0.1575 Avg Loss = 0.0797 Time = 0.0716s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 400/1137] Loss = 0.09443 Avg Loss = 0.08241 Time = 0.0616s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 450/1137] Loss = 0.0504 Avg Loss = 0.07528 Time = 0.0824s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 500/1137] Loss = 0.01831 Avg Loss = 0.07676 Time = 0.0374s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 550/1137] Loss = 0.09642 Avg Loss = 0.07275 Time = 0.0583s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 600/1137] Loss = 0.02182 Avg Loss = 0.0716 Time = 0.0772s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 650/1137] Loss = 0.06548 Avg Loss = 0.07212 Time = 0.0611s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 700/1137] Loss = 0.02436 Avg Loss = 0.07405 Time = 0.0534s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 750/1137] Loss = 0.02064 Avg Loss = 0.07315 Time = 0.0698s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 800/1137] Loss = 0.02428 Avg Loss = 0.07192 Time = 0.0631s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 850/1137] Loss = 0.2555 Avg Loss = 0.07289 Time = 0.0716s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 900/1137] Loss = 0.0603 Avg Loss = 0.07668 Time = 0.0634s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [ 950/1137] Loss = 0.03896 Avg Loss = 0.07665 Time = 0.0388s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [1000/1137] Loss = 0.0742 Avg Loss = 0.08043 Time = 0.0697s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [1050/1137] Loss = 0.05336 Avg Loss = 0.08044 Time = 0.0435s\n",
            "INFO:root:Epoch = [  2/ 10] Iter = [1100/1137] Loss = 0.03719 Avg Loss = 0.0716 Time = 0.0570s\n",
            "INFO:root:Epoch = [   2/  10] TrainLoss = 0.07071 ValLoss = 0.01543 TrainTime = 72.1913s ValTime = 4.8635s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [   0/1137] Loss = 0.2549 Avg Loss = 0.2549 Time = 0.7903s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [  50/1137] Loss = 0.07373 Avg Loss = 0.1824 Time = 0.0581s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 100/1137] Loss = 0.07775 Avg Loss = 0.1435 Time = 0.0668s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 150/1137] Loss = 0.256 Avg Loss = 0.1206 Time = 0.0460s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 200/1137] Loss = 0.01085 Avg Loss = 0.1034 Time = 0.0560s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 250/1137] Loss = 0.06318 Avg Loss = 0.0929 Time = 0.0602s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 300/1137] Loss = 0.08179 Avg Loss = 0.08684 Time = 0.0790s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 350/1137] Loss = 0.08037 Avg Loss = 0.0873 Time = 0.0596s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 400/1137] Loss = 0.1126 Avg Loss = 0.07891 Time = 0.0579s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 450/1137] Loss = 0.05919 Avg Loss = 0.07834 Time = 0.0765s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 500/1137] Loss = 0.0762 Avg Loss = 0.07928 Time = 0.0743s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 550/1137] Loss = 0.2589 Avg Loss = 0.08005 Time = 0.0543s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 600/1137] Loss = 0.09779 Avg Loss = 0.07954 Time = 0.0630s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 650/1137] Loss = 0.03964 Avg Loss = 0.07886 Time = 0.0692s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 700/1137] Loss = 0.01618 Avg Loss = 0.06892 Time = 0.1202s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 750/1137] Loss = 0.01332 Avg Loss = 0.07589 Time = 0.0527s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 800/1137] Loss = 0.0754 Avg Loss = 0.07197 Time = 0.0553s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 850/1137] Loss = 0.01462 Avg Loss = 0.07141 Time = 0.0682s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 900/1137] Loss = 0.06527 Avg Loss = 0.07582 Time = 0.0597s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [ 950/1137] Loss = 0.1103 Avg Loss = 0.07944 Time = 0.0469s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [1000/1137] Loss = 0.03467 Avg Loss = 0.0752 Time = 0.0548s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [1050/1137] Loss = 0.07332 Avg Loss = 0.07686 Time = 0.0826s\n",
            "INFO:root:Epoch = [  3/ 10] Iter = [1100/1137] Loss = 0.1427 Avg Loss = 0.08083 Time = 0.0581s\n",
            "INFO:root:Epoch = [   3/  10] TrainLoss = 0.07339 ValLoss = 0.02354 TrainTime = 72.3990s ValTime = 4.9532s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [   0/1137] Loss = 0.08507 Avg Loss = 0.08507 Time = 0.8447s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [  50/1137] Loss = 0.07681 Avg Loss = 0.08551 Time = 0.0643s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 100/1137] Loss = 0.06069 Avg Loss = 0.08951 Time = 0.0631s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 150/1137] Loss = 0.08469 Avg Loss = 0.08672 Time = 0.0487s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 200/1137] Loss = 0.1021 Avg Loss = 0.07567 Time = 0.0541s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 250/1137] Loss = 0.06266 Avg Loss = 0.07153 Time = 0.0565s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 300/1137] Loss = 0.143 Avg Loss = 0.07103 Time = 0.0812s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 350/1137] Loss = 0.1652 Avg Loss = 0.0728 Time = 0.0532s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 400/1137] Loss = 0.08027 Avg Loss = 0.07329 Time = 0.0739s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 450/1137] Loss = 0.1358 Avg Loss = 0.07057 Time = 0.0765s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 500/1137] Loss = 0.1495 Avg Loss = 0.07225 Time = 0.0576s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 550/1137] Loss = 0.03174 Avg Loss = 0.07482 Time = 0.0566s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 600/1137] Loss = 0.06691 Avg Loss = 0.06843 Time = 0.0590s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 650/1137] Loss = 0.07162 Avg Loss = 0.07119 Time = 0.0711s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 700/1137] Loss = 0.05101 Avg Loss = 0.07169 Time = 0.0683s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 750/1137] Loss = 0.01371 Avg Loss = 0.06911 Time = 0.0431s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 800/1137] Loss = 0.005137 Avg Loss = 0.06257 Time = 0.0544s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 850/1137] Loss = 0.01363 Avg Loss = 0.06388 Time = 0.0577s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 900/1137] Loss = 0.06012 Avg Loss = 0.06561 Time = 0.0734s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [ 950/1137] Loss = 0.08561 Avg Loss = 0.06298 Time = 0.0730s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [1000/1137] Loss = 0.008162 Avg Loss = 0.06498 Time = 0.0782s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [1050/1137] Loss = 0.0579 Avg Loss = 0.0693 Time = 0.0567s\n",
            "INFO:root:Epoch = [  4/ 10] Iter = [1100/1137] Loss = 0.07761 Avg Loss = 0.07535 Time = 0.0666s\n",
            "INFO:root:Epoch = [   4/  10] TrainLoss = 0.07312 ValLoss = 0.03491 TrainTime = 72.1864s ValTime = 4.9471s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [   0/1137] Loss = 0.3143 Avg Loss = 0.3143 Time = 0.8456s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [  50/1137] Loss = 0.07911 Avg Loss = 0.22 Time = 0.0532s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 100/1137] Loss = 0.09822 Avg Loss = 0.1634 Time = 0.0838s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 150/1137] Loss = 0.07775 Avg Loss = 0.1258 Time = 0.0791s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 200/1137] Loss = 0.1625 Avg Loss = 0.1088 Time = 0.0611s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 250/1137] Loss = 0.01996 Avg Loss = 0.09437 Time = 0.0734s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 300/1137] Loss = 0.0568 Avg Loss = 0.08989 Time = 0.0589s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 350/1137] Loss = 0.1367 Avg Loss = 0.08763 Time = 0.0661s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 400/1137] Loss = 0.01421 Avg Loss = 0.08644 Time = 0.0588s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 450/1137] Loss = 0.1314 Avg Loss = 0.08055 Time = 0.0740s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 500/1137] Loss = 0.06949 Avg Loss = 0.07836 Time = 0.0750s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 550/1137] Loss = 0.07161 Avg Loss = 0.07152 Time = 0.0535s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 600/1137] Loss = 0.07907 Avg Loss = 0.0706 Time = 0.0464s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 650/1137] Loss = 0.06484 Avg Loss = 0.07141 Time = 0.0374s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 700/1137] Loss = 0.1216 Avg Loss = 0.07106 Time = 0.0567s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 750/1137] Loss = 0.2355 Avg Loss = 0.07749 Time = 0.0526s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 800/1137] Loss = 0.05835 Avg Loss = 0.07798 Time = 0.0704s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 850/1137] Loss = 0.05054 Avg Loss = 0.07356 Time = 0.0350s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 900/1137] Loss = 0.05026 Avg Loss = 0.07296 Time = 0.0771s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [ 950/1137] Loss = 0.0464 Avg Loss = 0.06973 Time = 0.0739s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [1000/1137] Loss = 0.06361 Avg Loss = 0.06918 Time = 0.0707s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [1050/1137] Loss = 0.1144 Avg Loss = 0.07145 Time = 0.0578s\n",
            "INFO:root:Epoch = [  5/ 10] Iter = [1100/1137] Loss = 0.05783 Avg Loss = 0.06931 Time = 0.0644s\n",
            "INFO:root:Epoch = [   5/  10] TrainLoss = 0.06945 ValLoss = 0.02249 TrainTime = 72.0981s ValTime = 5.0511s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [   0/1137] Loss = 0.09923 Avg Loss = 0.09923 Time = 0.8348s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [  50/1137] Loss = 0.02193 Avg Loss = 0.08627 Time = 0.0564s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 100/1137] Loss = 0.03059 Avg Loss = 0.0782 Time = 0.0760s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 150/1137] Loss = 0.2084 Avg Loss = 0.07469 Time = 0.0603s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 200/1137] Loss = 0.09003 Avg Loss = 0.07156 Time = 0.0710s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 250/1137] Loss = 0.05201 Avg Loss = 0.07464 Time = 0.0541s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 300/1137] Loss = 0.01083 Avg Loss = 0.07197 Time = 0.0777s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 350/1137] Loss = 0.09413 Avg Loss = 0.07277 Time = 0.0922s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 400/1137] Loss = 0.01234 Avg Loss = 0.07195 Time = 0.0534s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 450/1137] Loss = 0.04604 Avg Loss = 0.06729 Time = 0.0794s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 500/1137] Loss = 0.05868 Avg Loss = 0.07198 Time = 0.0709s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 550/1137] Loss = 0.01494 Avg Loss = 0.07365 Time = 0.0617s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 600/1137] Loss = 0.03132 Avg Loss = 0.07769 Time = 0.0597s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 650/1137] Loss = 0.1001 Avg Loss = 0.07856 Time = 0.0629s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 700/1137] Loss = 0.05953 Avg Loss = 0.08153 Time = 0.0678s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 750/1137] Loss = 0.08648 Avg Loss = 0.07456 Time = 0.0878s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 800/1137] Loss = 0.03442 Avg Loss = 0.06672 Time = 0.0776s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 850/1137] Loss = 0.02443 Avg Loss = 0.06513 Time = 0.0779s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 900/1137] Loss = 0.04544 Avg Loss = 0.06488 Time = 0.0667s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [ 950/1137] Loss = 0.05012 Avg Loss = 0.06374 Time = 0.0624s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [1000/1137] Loss = 0.04566 Avg Loss = 0.06844 Time = 0.0775s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [1050/1137] Loss = 0.2391 Avg Loss = 0.0698 Time = 0.0793s\n",
            "INFO:root:Epoch = [  6/ 10] Iter = [1100/1137] Loss = 0.03937 Avg Loss = 0.07659 Time = 0.0606s\n",
            "INFO:root:Epoch = [   6/  10] TrainLoss = 0.07185 ValLoss = 0.03174 TrainTime = 71.7896s ValTime = 4.9078s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [   0/1137] Loss = 0.03571 Avg Loss = 0.03571 Time = 0.8262s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [  50/1137] Loss = 0.01062 Avg Loss = 0.04801 Time = 0.0828s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 100/1137] Loss = 0.04293 Avg Loss = 0.05272 Time = 0.0842s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 150/1137] Loss = 0.06374 Avg Loss = 0.06109 Time = 0.0639s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 200/1137] Loss = 0.1131 Avg Loss = 0.06888 Time = 0.0724s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 250/1137] Loss = 0.1097 Avg Loss = 0.06677 Time = 0.0716s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 300/1137] Loss = 0.1121 Avg Loss = 0.06865 Time = 0.0533s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 350/1137] Loss = 0.008874 Avg Loss = 0.06799 Time = 0.0594s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 400/1137] Loss = 0.06608 Avg Loss = 0.06569 Time = 0.0374s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 450/1137] Loss = 0.01073 Avg Loss = 0.06467 Time = 0.0755s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 500/1137] Loss = 0.08242 Avg Loss = 0.06712 Time = 0.0773s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 550/1137] Loss = 0.02939 Avg Loss = 0.06631 Time = 0.0518s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 600/1137] Loss = 0.04486 Avg Loss = 0.06551 Time = 0.0649s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 650/1137] Loss = 0.0627 Avg Loss = 0.06936 Time = 0.0652s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 700/1137] Loss = 0.01002 Avg Loss = 0.06798 Time = 0.0775s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 750/1137] Loss = 0.0655 Avg Loss = 0.06708 Time = 0.0778s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 800/1137] Loss = 0.0687 Avg Loss = 0.06732 Time = 0.0659s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 850/1137] Loss = 0.07863 Avg Loss = 0.0649 Time = 0.0350s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 900/1137] Loss = 0.08323 Avg Loss = 0.06982 Time = 0.0732s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [ 950/1137] Loss = 0.1664 Avg Loss = 0.07043 Time = 0.0614s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [1000/1137] Loss = 0.03681 Avg Loss = 0.06876 Time = 0.0610s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [1050/1137] Loss = 0.1361 Avg Loss = 0.07052 Time = 0.0598s\n",
            "INFO:root:Epoch = [  7/ 10] Iter = [1100/1137] Loss = 0.1637 Avg Loss = 0.07273 Time = 0.0738s\n",
            "INFO:root:Epoch = [   7/  10] TrainLoss = 0.06535 ValLoss = 0.04075 TrainTime = 72.4687s ValTime = 4.9137s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [   0/1137] Loss = 0.03373 Avg Loss = 0.03373 Time = 0.6860s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [  50/1137] Loss = 0.02954 Avg Loss = 0.04093 Time = 0.0628s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 100/1137] Loss = 0.0334 Avg Loss = 0.04892 Time = 0.0756s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 150/1137] Loss = 0.03211 Avg Loss = 0.05286 Time = 0.0398s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 200/1137] Loss = 0.03833 Avg Loss = 0.05802 Time = 0.0442s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 250/1137] Loss = 0.008091 Avg Loss = 0.05734 Time = 0.0582s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 300/1137] Loss = 0.01808 Avg Loss = 0.05588 Time = 0.0812s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 350/1137] Loss = 0.06527 Avg Loss = 0.06005 Time = 0.0608s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 400/1137] Loss = 0.08608 Avg Loss = 0.06162 Time = 0.0775s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 450/1137] Loss = 0.02926 Avg Loss = 0.05823 Time = 0.0576s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 500/1137] Loss = 0.0646 Avg Loss = 0.06504 Time = 0.0637s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 550/1137] Loss = 0.03274 Avg Loss = 0.06159 Time = 0.0582s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 600/1137] Loss = 0.0506 Avg Loss = 0.06232 Time = 0.0691s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 650/1137] Loss = 0.0452 Avg Loss = 0.06012 Time = 0.0373s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 700/1137] Loss = 0.09849 Avg Loss = 0.06139 Time = 0.0488s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 750/1137] Loss = 0.07024 Avg Loss = 0.06461 Time = 0.0528s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 800/1137] Loss = 0.07872 Avg Loss = 0.06327 Time = 0.0691s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 850/1137] Loss = 0.01782 Avg Loss = 0.06167 Time = 0.0595s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 900/1137] Loss = 0.02607 Avg Loss = 0.0633 Time = 0.0535s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [ 950/1137] Loss = 0.05646 Avg Loss = 0.06218 Time = 0.0693s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [1000/1137] Loss = 0.02107 Avg Loss = 0.0625 Time = 0.0710s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [1050/1137] Loss = 0.03166 Avg Loss = 0.0661 Time = 0.0550s\n",
            "INFO:root:Epoch = [  8/ 10] Iter = [1100/1137] Loss = 0.00467 Avg Loss = 0.05822 Time = 0.0604s\n",
            "INFO:root:Epoch = [   8/  10] TrainLoss = 0.05518 ValLoss = 0.03597 TrainTime = 72.1950s ValTime = 4.9051s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [   0/1137] Loss = 0.1039 Avg Loss = 0.1039 Time = 0.8362s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [  50/1137] Loss = 0.05848 Avg Loss = 0.08852 Time = 0.0371s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 100/1137] Loss = 0.001825 Avg Loss = 0.07998 Time = 0.0552s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 150/1137] Loss = 0.06306 Avg Loss = 0.07789 Time = 0.0810s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 200/1137] Loss = 0.001068 Avg Loss = 0.06917 Time = 0.0588s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 250/1137] Loss = 0.00283 Avg Loss = 0.06254 Time = 0.0656s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 300/1137] Loss = 0.1083 Avg Loss = 0.06311 Time = 0.0732s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 350/1137] Loss = 0.06491 Avg Loss = 0.05759 Time = 0.0584s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 400/1137] Loss = 0.216 Avg Loss = 0.0607 Time = 0.0384s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 450/1137] Loss = 0.1464 Avg Loss = 0.0639 Time = 0.0588s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 500/1137] Loss = 0.04333 Avg Loss = 0.07021 Time = 0.0841s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 550/1137] Loss = 0.06058 Avg Loss = 0.06438 Time = 0.0549s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 600/1137] Loss = 0.06353 Avg Loss = 0.06328 Time = 0.0754s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 650/1137] Loss = 0.04419 Avg Loss = 0.05976 Time = 0.0605s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 700/1137] Loss = 0.02743 Avg Loss = 0.06022 Time = 0.0583s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 750/1137] Loss = 0.07212 Avg Loss = 0.05778 Time = 0.0696s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 800/1137] Loss = 0.002906 Avg Loss = 0.06309 Time = 0.0563s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 850/1137] Loss = 0.01604 Avg Loss = 0.05682 Time = 0.0583s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 900/1137] Loss = 0.02307 Avg Loss = 0.05788 Time = 0.0352s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [ 950/1137] Loss = 0.09773 Avg Loss = 0.05832 Time = 0.0727s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [1000/1137] Loss = 0.03046 Avg Loss = 0.0599 Time = 0.0592s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [1050/1137] Loss = 0.01271 Avg Loss = 0.05848 Time = 0.0748s\n",
            "INFO:root:Epoch = [  9/ 10] Iter = [1100/1137] Loss = 0.0467 Avg Loss = 0.06116 Time = 0.0758s\n",
            "INFO:root:Epoch = [   9/  10] TrainLoss = 0.06702 ValLoss = 0.004923 TrainTime = 72.2289s ValTime = 4.8795s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vTQCfsJ70wI"
      },
      "source": [
        "## val"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WEvB-_77z1E"
      },
      "source": [
        "!python '/content/gdrive/My Drive/WIP/EIEN-2020-Fall/EIEN394/Lecture_for_me/Lecture 17 - Exp/Toy code for segmentation/Segmentation_using_DL/Segm_DL_playground-master_v3/test.py' --data-path '/content/gdrive/My Drive/WIP/EIEN-2020-Fall/EIEN394/Lecture_for_me/Lecture 17 - Exp/Toy code for segmentation/TrainingData_Splitted' --data-split val --checkpoint '/content/gdrive/MyDrive/WIP/EIEN-2020-Fall/EIEN394/Lecture_for_me/Lecture 17 - Exp/Toy code for segmentation/Segmentation_using_DL/ckt/2020_11_17_00_56_44/best_model.pt' --out-dir '/content/gdrive/My Drive/WIP/EIEN-2020-Fall/EIEN394/Lecture_for_me/Lecture 17 - Exp/Toy code for segmentation/Segmentation_using_DL/out_dir'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74LDC0IJ77fT"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym-TJ1oI-Bnw"
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6YGFl5s785I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "2aa737cf-6fd3-4273-f94e-f69fc8de3a2b"
      },
      "source": [
        "file = '/content/gdrive/MyDrive/WIP/EIEN-2020-Fall/EIEN394/Lecture_for_me/Lecture 17 - Exp/Toy code for segmentation/Segmentation_using_DL/out_dir/2020_11_17_01_20_28/PROMISE12_val_data_41.h5'\n",
        "hf = h5py.File(file)\n",
        "print('Keys:', list(hf.keys()))\n",
        "print('Attrs:', dict(hf.attrs))\n",
        "current_img = hf['reconstruction'][()]\n",
        "print('shape of current_img', current_img.shape, current_img.dtype)\n",
        "plt.figure()\n",
        "plt.imshow(np.abs(current_img[7,0,:,:]), cmap='gray')\n",
        "#plt.savefig('/content/gdrive/My Drive/WIP/pytorch code - unsMBTR/code_maskgen_v1/fig1.png', dpi=300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Keys: ['reconstruction']\n",
            "Attrs: {}\n",
            "shape of current_img (24, 1, 256, 256) int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f991da68278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}